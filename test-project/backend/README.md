# Backend - Guide DÃ©veloppeur

Ce guide vous aidera Ã  comprendre et dÃ©velopper le backend Python de l'application TriDyme.

## Table des matiÃ¨res

- [Installation](#installation)
- [Architecture](#architecture)
- [Tests unitaires](#tests-unitaires)
- [API Documentation](#api-documentation)
- [DÃ©veloppement](#dÃ©veloppement)

## <a name="installation"></a>Installation

### 1/ CrÃ©ation d'un environnement virtuel

```shell
$ python3 -m venv env
```

### 2/ Activation de l'environnement virtuel

Sur Mac/Linux :
```shell
$ source env/bin/activate
```

Sur Windows :
```shell
$ .\env\Scripts\activate
```

### 3/ Installation des librairies

Mettre Ã  jour pip:

```shell
$ pip install --upgrade pip
```

```shell
$ python3 -m pip install -r requirements.txt
```

### 4/ Lancement du serveur

```shell
$ python3 main.py
```

Le serveur sera accessible sur `http://localhost:8000`

## <a name="architecture"></a>Architecture

```
backend/
â”œâ”€â”€ api.py              # Routes API et logique mÃ©tier
â”œâ”€â”€ main.py             # Point d'entrÃ©e de l'application
â”œâ”€â”€ requirements.txt    # DÃ©pendances Python
â”œâ”€â”€ pytest.ini         # Configuration des tests
â””â”€â”€ tests/              # Tests unitaires
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_api.py     # Tests des routes API
```

## <a name="tests-unitaires"></a>Tests unitaires

### Pourquoi tester ?

Les tests unitaires permettent de :
- ğŸ” **DÃ©tecter les bugs** avant la mise en production
- ğŸ›¡ï¸ **PrÃ©venir les rÃ©gressions** lors des modifications
- ğŸ“š **Documenter** le comportement attendu du code
- ğŸš€ **Faciliter le dÃ©veloppement** en Ã©quipe

### Framework de test : pytest

Ce projet utilise **pytest**, un framework de test Python moderne et puissant.

### Installation des dÃ©pendances de test

Les dÃ©pendances de test sont dÃ©jÃ  incluses dans `requirements.txt` :
```
pytest          # Framework de test
pytest-cov     # Couverture de code
httpx          # Client HTTP pour tester FastAPI
```

### Structure d'un test

Un test suit cette structure de base :

```python
def test_nom_du_test():
    # Arrange - PrÃ©parer les donnÃ©es
    test_data = {"key": "value"}
    
    # Act - Executer l'action Ã  tester
    response = client.post("/api/endpoint", json=test_data)
    
    # Assert - VÃ©rifier le rÃ©sultat
    assert response.status_code == 200
    assert response.json()["result"] == "expected_value"
```

### Exemple concret : Tester une fonction de calcul

Supposons que vous avez une fonction de calcul d'inertie :

```python
# Dans votre fichier de logique mÃ©tier
def calculate_inertia(width, height):
    """Calcule l'inertie d'une section rectangulaire"""
    if width <= 0 or height <= 0:
        raise ValueError("Les dimensions doivent Ãªtre positives")
    
    return (width * height**3) / 12
```

Voici comment la tester :

```python
# Dans tests/test_calculations.py
import pytest
from calculations import calculate_inertia

class TestCalculateInertia:
    """Tests pour la fonction calculate_inertia"""
    
    def test_inertia_basic_calculation(self):
        """Test du calcul d'inertie basique"""
        # Arrange
        width = 10
        height = 20
        expected = (10 * 20**3) / 12  # 6666.67
        
        # Act
        result = calculate_inertia(width, height)
        
        # Assert
        assert abs(result - expected) < 0.01  # Comparaison avec tolÃ©rance
    
    def test_inertia_with_zero_width(self):
        """Test avec largeur nulle - doit lever une exception"""
        with pytest.raises(ValueError, match="dimensions doivent Ãªtre positives"):
            calculate_inertia(0, 10)
    
    def test_inertia_with_negative_height(self):
        """Test avec hauteur nÃ©gative - doit lever une exception"""
        with pytest.raises(ValueError):
            calculate_inertia(10, -5)
    
    @pytest.mark.parametrize("width,height,expected", [
        (1, 1, 1/12),
        (2, 4, (2 * 4**3) / 12),
        (5, 3, (5 * 3**3) / 12),
    ])
    def test_inertia_multiple_cases(self, width, height, expected):
        """Test avec plusieurs jeux de donnÃ©es"""
        result = calculate_inertia(width, height)
        assert abs(result - expected) < 0.01
```

### Tester les routes API

Exemple simple pour tester la route `/analysis` :

```python
# Dans tests/test_api.py
import pytest
from fastapi.testclient import TestClient
from api import router
from fastapi import FastAPI

@pytest.fixture
def app():
    """Fixture pour crÃ©er l'application FastAPI"""
    app = FastAPI()
    app.include_router(router)
    return app

@pytest.fixture
def client(app):
    """Fixture pour crÃ©er le client de test"""
    return TestClient(app)

class TestAnalysisRoute:
    """Tests pour la route /analysis"""
    
    def test_analysis_basic_calculation(self, client):
        """Test basique de la route /analysis avec calcul simple"""
        # Arrange - PrÃ©parer les donnÃ©es de test
        test_data = {
            "length": {"value": 100},
            "width": {"value": 50}
        }
        
        # Act - ExÃ©cuter la requÃªte
        response = client.post("/api/analysis", json=test_data)
        
        # Assert - VÃ©rifier les rÃ©sultats
        assert response.status_code == 200
        result = response.json()
        assert result["length"]["value"] == 110  # 100 + 10 (logique mÃ©tier)
        assert result["width"]["value"] == 50  # InchangÃ©
```

### Lancement des tests

#### Tests de base
```shell
$ python -m pytest tests/ --verbose
```

#### Tests avec couverture de code
```shell
$ python -m pytest tests/ --cov=. --cov-report=term-missing --verbose
```

#### Tests d'un fichier spÃ©cifique
```shell
$ python -m pytest tests/test_api.py --verbose
```

#### Tests d'une fonction spÃ©cifique
```shell
$ python -m pytest tests/test_api.py::test_analysis_route_success --verbose
```

### Bonnes pratiques

#### 1. Nommage des tests
- Utilisez des noms explicites : `test_calculate_inertia_with_valid_inputs`
- Suivez le pattern : `test_[fonction]_[condition]_[rÃ©sultat_attendu]`

#### 2. Organisation des tests
- Une classe de test par fonction/module testÃ©
- Grouper les tests par fonctionnalitÃ©

#### 3. Structure Arrange-Act-Assert
```python
def test_example():
    # Arrange - PrÃ©parer les donnÃ©es
    input_data = {"key": "value"}
    
    # Act - Executer l'action
    result = function_to_test(input_data)
    
    # Assert - VÃ©rifier le rÃ©sultat
    assert result == expected_value
```

#### 4. Tests des cas limites
Testez toujours :
- âœ… Les cas normaux
- âœ… Les cas limites (valeurs nulles, nÃ©gatives, trÃ¨s grandes)
- âœ… Les cas d'erreur (exceptions attendues)

#### 5. Fixtures pour la rÃ©utilisabilitÃ©
```python
@pytest.fixture
def sample_data():
    """DonnÃ©es de test rÃ©utilisables"""
    return {
        "width": {"value": 100},
        "height": {"value": 200}
    }

def test_with_fixture(sample_data):
    # Utilise les donnÃ©es de la fixture
    assert sample_data["width"]["value"] == 100
```

### IntÃ©gration CI/CD

Les tests sont automatiquement exÃ©cutÃ©s dans la pipeline GitLab CI/CD :

1. **Installation** des dÃ©pendances
2. **ExÃ©cution** des tests avec `pytest`
3. **GÃ©nÃ©ration** du rapport de couverture
4. **Ã‰chec** du pipeline si les tests ne passent pas

### DÃ©buggage des tests

Pour dÃ©bugger un test qui Ã©choue :

```shell
# Afficher plus d'informations
$ python -m pytest tests/test_api.py::test_failing_test -v -s

# ArrÃªter au premier Ã©chec
$ python -m pytest tests/ -x

# Entrer en mode debug
$ python -m pytest tests/ --pdb
```

## <a name="api-documentation"></a>API Documentation

La documentation interactive de l'API est disponible sur :
- Swagger UI : `http://localhost:8000/docs`
- ReDoc : `http://localhost:8000/redoc`

## <a name="dÃ©veloppement"></a>DÃ©veloppement

### Ajouter une nouvelle route

1. **DÃ©finir** la route dans `api.py`
2. **Ã‰crire** les tests dans `tests/test_api.py`
3. **ExÃ©cuter** les tests pour vÃ©rifier
4. **Documenter** la route avec des docstrings

### Workflow de dÃ©veloppement

1. ğŸ” **Comprendre** le besoin
2. âœï¸ **Ã‰crire** le test (TDD - Test Driven Development)
3. ğŸ’» **ImplÃ©menter** la fonctionnalitÃ©
4. âœ… **VÃ©rifier** que le test passe
5. ğŸ”„ **Refactoriser** si nÃ©cessaire
6. ğŸ“ **Documenter** le code

Cette approche garantit une qualitÃ© de code Ã©levÃ©e et facilite la maintenance.
