# D√©finition des stages
stages:
  - install
  - build
  - test
  - lint
  - docker
  - deploy

# Variables globales
variables:
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: '/certs'
  GCP_PROJECT_ID: '${GCP_PROJECT_ID}'
  GKE_CLUSTER_NAME: '${GKE_CLUSTER_NAME}'
  GKE_CLUSTER_ZONE: '${GKE_CLUSTER_ZONE}'
  K8S_NAMESPACE: '${K8S_NAMESPACE:-default}'

# Cache pour optimiser les builds
cache:
  key: '$CI_COMMIT_REF_SLUG'
  paths:
    - frontend/node_modules/
    - frontend/module-federation/node_modules/
    - backend/env/

# Installation des d√©pendances Frontend
install:frontend:
  stage: install
  image: node:18-alpine
  # tags: supprim√© pour utiliser les shared runners
  script:
    - cd frontend
    - npm ci --cache .npm --prefer-offline
    - cd module-federation
    - npm ci --cache .npm --prefer-offline
  artifacts:
    paths:
      - frontend/node_modules/
      - frontend/module-federation/node_modules/
    expire_in: 1 hour

# Installation des d√©pendances Backend
install:backend:
  stage: install
  image: python:3.12-slim
  # tags: supprim√© pour utiliser les shared runners
  script:
    - cd backend
    - python -m venv env
    - source env/bin/activate
    - pip install --upgrade pip
    - pip install -r requirements.txt
  artifacts:
    paths:
      - backend/env/
    expire_in: 1 hour

# Build du frontend
build:frontend:
  stage: build
  image: node:18-alpine
  # tags: supprim√© pour utiliser les shared runners
  variables:
    CI: 'false'
  dependencies:
    - install:frontend
  script:
    - cd frontend
    - npm run build
  artifacts:
    paths:
      - frontend/build/
    expire_in: 1 hour

# Tests Backend
test:backend:
  stage: test
  image: python:3.12-slim
  # tags: supprim√© pour utiliser les shared runners
  dependencies:
    - install:backend
  script:
    - cd backend
    - source env/bin/activate
    - echo "üß™ Ex√©cution des tests backend..."
    - python -m pytest tests/ --cov=. --cov-report=xml --cov-report=term --junit-xml=backend/junit.xml --verbose
    - echo "‚úÖ Tests backend termin√©s"
  artifacts:
    paths:
      - backend/coverage.xml
      - backend/junit.xml
    reports:
      junit: backend/junit.xml
    expire_in: 1 week
  allow_failure: false

# Tests Frontend
test:frontend:
  stage: test
  image: node:18-alpine
  # tags: supprim√© pour utiliser les shared runners
  dependencies:
    - install:frontend
  script:
    - cd frontend
    - echo "Tests frontend - placeholder"
    # - npm test -- --coverage --watchAll=false
  allow_failure: true

# Linting Frontend
lint:frontend:
  stage: lint
  image: node:18-alpine
  # tags: supprim√© pour utiliser les shared runners
  dependencies:
    - install:frontend
  script:
    - cd frontend
    - echo "Linting - placeholder"
    # - npx eslint src --ext .js,.jsx,.ts,.tsx
  allow_failure: true

# Construction de l'image Docker
docker:build:
  stage: docker
  image:
    name: gcr.io/kaniko-project/executor:v1.9.0-debug
    entrypoint: ['']
  # tags: supprim√© pour utiliser les shared runners
  dependencies:
    - build:frontend
  before_script:
    - echo "{\"auths\":{\"$CI_REGISTRY\":{\"username\":\"$CI_REGISTRY_USER\",\"password\":\"$CI_REGISTRY_PASSWORD\"}}}" > /kaniko/.docker/config.json
  script:
    - /kaniko/executor
      --context "${CI_PROJECT_DIR}"
      --dockerfile "Dockerfile"
      --destination "${CI_REGISTRY_IMAGE}:${CI_COMMIT_SHA}"
      --destination "${CI_REGISTRY_IMAGE}:latest"
      --cleanup
      --cache=true
      --cache-ttl=168h
  only:
    - main
    - develop

# D√©ploiement sur GKE avec debug am√©lior√©

deploy:gke:
  stage: deploy
  image: google/cloud-sdk:latest
  dependencies: []
  before_script:
    # V√©rification de la cl√© GCP (maintenant un fichier)
    - echo "üîç V√©rification de la cl√© GCP..."
    - |
      if [ ! -f "$GCP_SERVICE_ACCOUNT_KEY" ]; then
        echo "‚ùå ERROR: Fichier GCP_SERVICE_ACCOUNT_KEY non trouv√©"
        exit 1
      else
        echo "‚úÖ Fichier cl√© trouv√©: $GCP_SERVICE_ACCOUNT_KEY"
        echo "Taille: $(wc -c < "$GCP_SERVICE_ACCOUNT_KEY") bytes"
      fi

    # Validation JSON directe
    - echo "üß™ Validation JSON..."
    - |
      python3 -c "import json; data=json.load(open('$GCP_SERVICE_ACCOUNT_KEY')); print(f'‚úÖ JSON valide - Project: {data[\"project_id\"]}')"

    # Authentification GCP (directe, sans d√©codage)
    - echo "üîê Authentification GCP..."
    - gcloud auth activate-service-account --key-file="$GCP_SERVICE_ACCOUNT_KEY"
    - gcloud config set project $GCP_PROJECT_ID
    - gcloud container clusters get-credentials $GKE_CLUSTER_NAME --zone $GKE_CLUSTER_ZONE

    # Installation kubectl
    - |
      if ! command -v kubectl &> /dev/null; then
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl && mv kubectl /usr/local/bin/
      fi

  script:
    - echo "üöÄ D√©ploiement..."

    # Cr√©ation des manifests si absents
    - |
      if [ ! -d "k8s" ]; then
        mkdir -p k8s
        cat > k8s/deployment.yaml << 'EOF'
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ec2-ferraillage-app
        labels:
          app: ec2-ferraillage
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: ec2-ferraillage
        template:
          metadata:
            labels:
              app: ec2-ferraillage
          spec:
            containers:
            - name: app
              image: ${CI_REGISTRY_IMAGE}:${CI_COMMIT_SHA}
              ports:
              - containerPort: 8000
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: ec2-ferraillage-service
      spec:
        type: ClusterIP
        ports:
        - port: 80
          targetPort: 8000
        selector:
          app: ec2-ferraillage
      EOF
      fi

    # Mise √† jour des images
    - sed -i "s|\${CI_REGISTRY_IMAGE}:\${CI_COMMIT_SHA}|${CI_REGISTRY_IMAGE}:${CI_COMMIT_SHA}|g" k8s/*.yaml

    # D√©ploiement
    - kubectl apply -f k8s/ -n ${K8S_NAMESPACE:-default}
    - kubectl rollout status deployment/ec2-ferraillage-app -n ${K8S_NAMESPACE:-default} --timeout=300s
    - kubectl get services -n ${K8S_NAMESPACE:-default}

  environment:
    name: production
  when: manual
  only:
    - main
